---
title: "Paper I read"
tags:
  - deep learning
---

{% include toc title="Unique Title" icon="file-text" %}

# Paper-I-read
This repo contains scientific paper I read as a reminder to myself. Hope this is helpful to you too.

# Time line

## 2017

[Mastering the game of Go without human knowledge (AlphaGO Zero)](https://www.nature.com/articles/nature24270.epdf?author_access_token=VJXbVjaSHxFoctQQ4p2k4tRgN0jAjWel9jnR3ZoTv0PVW4gB86EEpGqTRDtpIz-2rmo8-KG06gqVobU5NSCFeHILHcVFUeMsbvwS-lxjqQGg98faovwjxeTUgZAUMnRQ)

[Rainbow: Combining Improvements in Deep Reinforcement Learning (Rainbow 6)](https://yhyu13.github.io/DeepMind-Rainbow/)

[Residual LSTM: Design of a Deep Recurrent Architecture for Distant Speech
Recognition (Res-LSTM)](https://arxiv.org/pdf/1701.03360.pdf)

[Bridging the Gaps Between Residual Learning,
Recurrent Neural Networks and Visual Cortex (ResNet~=recurrent network)](https://arxiv.org/pdf/1604.03640.pdf)

[Reverse Curriculum Generation for Reinforcement Learning (Reward guiding)](https://arxiv.org/pdf/1707.05300.pdf)

[Automatic Goal Generation for Reinforcement Learning Agents](https://arxiv.org/pdf/1705.06366.pdf)

[Learning with Oppent-learning Awarness (LOLA)](https://arxiv.org/pdf/1709.04326.pdf)

[Neuroscience inspired A.I. (Deepmind)](http://www.cell.com/neuron/abstract/S0896-6273(17)30509-3)

[On the Origin of Deep Learning (CMU)](https://arxiv.org/pdf/1702.07800.pdf)

[Evolution Strategies as a
Scalable Alternative to Reinforcement Learning (OpenAi)](https://arxiv.org/pdf/1703.03864.pdf)

[Learning Hierarchical Information Flow with Recurrent Neural Modules (ThalNet)](https://pdfs.semanticscholar.org/76d1/9efb925b33e67d93c94a9cab242889186485.pdf)

[Learning human behaviors from motion capture by adversarial imitation (GAIL)](https://arxiv.org/pdf/1707.02201.pdf)

## 2016

[Deep Residual Networks with Exponential Linear Unit](https://arxiv.org/pdf/1604.04112.pdf)

[Residual Networks Behave Like Ensembles of
Relatively Shallow Networks](https://arxiv.org/pdf/1605.06431.pdf)

[Toward an Integration of Deep Learning and Neuroscience](http://journal.frontiersin.org/article/10.3389/fncom.2016.00094/full)

[Random synaptic feedback weights support error backpropagation for deep learning (Feedback alignment)](https://www.nature.com/articles/ncomms13276#s1)
[Supplementary material](https://images.nature.com/original/nature-assets/ncomms/2016/161108/ncomms13276/extref/ncomms13276-s1.pdf)

[How Does the Sparse Memory “Engram” Neurons Encode the Memory of a Spatial–Temporal Event? (Exitory-Inhibitory)](https://www.frontiersin.org/articles/10.3389/fncir.2016.00061/full)

[Memory-based control with recurrent neural networks (RDPG)](http://rll.berkeley.edu/deeprlworkshop/papers/rdpg.pdf)

[Asynchrouous Methods for Deep Reinforcement Learning (A3C)](https://arxiv.org/pdf/1602.01783.pdf)

## 2015

[A Statistical View of Deep Learning](http://blog.shakirm.com/wp-content/uploads/2015/07/SVDL.pdf)

## 2014

[On the Number of Linear Regions of Deep Neural Networks (Relu)](https://arxiv.org/pdf/1402.1869.pdf)

# Dessert 
[What is the statistical interpretation of NN - Quora (Regression equations)](https://www.quora.com/What-is-a-statistical-interpretation-of-neural-networks)

[RNN intro cs.toronto](http://www.cs.toronto.edu/~urtasun/courses/CSC2541_Winter17/RNN.pdf)

[14 DESIGN PATTERNS TO IMPROVE YOUR CONVOLUTIONAL NEURAL NETWORKS](https://www.topbots.com/14-design-patterns-improve-convolutional-neural-network-cnn-architecture/)
